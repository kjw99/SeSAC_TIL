## 웹 스크래핑

- 웹사이트의 HTML을 가져와서 원하는 정보만 골라내는 기술이다
- 뉴스 제목, 상품 가격, 날씨 정보 등 반복적으로 수집해야 하는 데이터에 활용된다
- 사람이 브라우저에서 하는 일을 코드로 자동화하는 것이다

### 크롤링 vs 스크래핑

- 크롤링 (Crawling)
    - 웹 페이지를 자동으로 돌아다니며 수집하는 행위이다
    - 예: 구글 검색엔진이 전 세계 웹사이트를 순회하며 색인한다
    - 넓고 자동화된 탐색이다
- 스크래핑 (Scraping)
    - 웹 페이지에서 원하는 데이터만 추출하는 행위이다
    - 예: 네이버 뉴스에서 기사 제목만 뽑아온다
    - 특정 페이지에서 정밀한 데이터를 추출한다

---

## BeautifulSoup을 활용한 정적 페이지 웹 스크래핑

### 동작 원리

```
1. 웹 페이지에 HTTP 요청을 보낸다  (requests)
2. HTML 응답을 받는다
3. HTML을 분석(파싱)한다           (BeautifulSoup)
4. 원하는 데이터를 추출한다
```

### 라이브러리 설치

```bash
pip install requests beautifulsoup4
```

- `beautifulsoup4` — 받아온 HTML을 분석(파싱)하여 원하는 데이터를 추출하는 역할이다

### HTML 가져오기

```python
import requests

url = "https://news.google.com/topics/CAAqIQgKIhtDQkFTRGdvSUwyMHZNRFp4WkRNU0FtdHZLQUFQAQ?hl=ko&gl=KR&ceid=KR%3Ako"

response = requests.get(url)

print(response.status_code)  # 200이면 성공

```

### 파싱하기

```python
from bs4 import BeautifulSoup

soup = BeautifulSoup(response.text, "html.parser")
```

- `response.text` — 서버에서 받아온 HTML 문자열이다
- `"html.parser"` — Python 내장 HTML 파서이다 (별도 설치 불필요)

---

### BeautifulSoup의 핵심 메서드

- `soup.select_one("선택자")` — 조건에 맞는 첫 번째 요소를 반환한다 (없으면 `None`)
- `soup.select("선택자")` — 조건에 맞는 모든 요소를 리스트로 반환한다

### 뉴스 제목 추출

- 개발자도구를 통해 문서의 구조를 파악한 뒤, 선택자를 통해 선택한다.

```python
# 뉴스 제목 링크 모두 가져오기
articles = soup.select("a.gPFEn")

for article in articles:
    title = article.get_text()       # 태그 안의 텍스트
    link = article["href"]           # href 속성값
    print(f"제목: {title}")
    print(f"링크: {link}")
    print("-" * 50)
```

- `.get_text()` — 태그 안의 텍스트만 추출한다 (예: `tag.get_text()` → `"기사 제목"`)
- `["속성명"]` — 특정 속성값을 가져온다 (예: `tag["href"]` → `"https://..."`)

### 전체 코드

```python
import requests
from bs4 import BeautifulSoup

url = "https://news.google.com/topics/CAAqIQgKIhtDQkFTRGdvSUwyMHZNRFp4WkRNU0FtdHZLQUFQAQ?hl=ko&gl=KR&ceid=KR%3Ako"

response = requests.get(url)

soup = BeautifulSoup(response.text, "html.parser")

articles = soup.select("a.gPFEn")

for article in articles:
    title = article.get_text()  # 태그 안의 텍스트
    link = article["href"]  # href 속성값
    print(f"제목: {title}")
    print(f"링크: {link}")
    print("-" * 50)

```

---

## Playwright를 활용한 동적 페이지 웹 스크래핑

### 정적 페이지 vs 동적 페이지

- 정적 페이지
    - HTML에 이미 모든 데이터가 들어 있다
    - `requests`로 받으면 바로 데이터를 추출할 수 있다
    - 예: 위키백과, 정부 공공데이터 페이지
- 동적 페이지
    - HTML은 빈 껍데기이고, JavaScript가 실행되면서 데이터가 채워진다
    - `requests`로 받으면 빈 페이지만 나온다
    - 예: 네이버 웹툰, 인스타그램, 쿠팡

- 최근 대부분의 웹사이트는 JavaScript로 콘텐츠를 동적으로 렌더링하며, 이런 페이지는 HTML을 받아도 내용이 비어 있다
- Playwright / Selenium등의 라이브러리는 실제 브라우저를 자동으로 조작하여 JavaScript가 실행된 후의 완성된 페이지를 가져올 수 있다

### 동작 원리

```
1. 브라우저를 실행한다 (Playwright / Selenium)
2. URL로 이동한다
3. JavaScript가 실행될 때까지 기다린다
4. 완성된 HTML에서 데이터를 추출한다
```

### 라이브러리 설치

```bash
pip install playwright
```

### 브라우저 설치

```bash
playwright install chromium
```

- Playwright는 별도의 브라우저를 다운로드하여 사용한다
- `chromium` — Chrome 기반 브라우저이다
- 한 번만 설치하면 된다

### 브라우저 열기

```python
from playwright.sync_api import sync_playwright

pw = sync_playwright().start()
browser = pw.chromium.launch(headless=False)
page = browser.new_page()
```

- `sync_playwright().start()` — Playwright를 시작한다
- `launch(headless=False)` — 브라우저 창을 보이게 실행한다
    - `headless=True`로 하면 브라우저 창 없이 백그라운드에서 실행된다
- `new_page()` — 새 탭을 연다

### 페이지 이동 및 대기

```python
page.goto("https://comic.naver.com/webtoon?tab=mon")
page.wait_for_load_state("networkidle")
```

- `goto()` — 해당 URL로 이동한다
- `wait_for_load_state("networkidle")` — 네트워크 요청이 끝날 때까지 기다린다
    - 동적 페이지는 이동 후 바로 데이터가 안 보일 수 있으므로 대기가 필요하다

### Playwright의 핵심 메서드

- `page.locator("선택자")` — CSS 선택자로 요소를 찾는다
- `locator.all()` — 조건에 맞는 모든 요소를 리스트로 반환한다
- `locator.first` — 첫 번째 요소를 반환한다
- `locator.text_content()` — 요소의 텍스트를 가져온다
- `locator.get_attribute("속성명")` — 요소의 속성값을 가져온다

### 웹툰 제목 추출

```python
# 웹툰 제목 요소 모두 가져오기
titles = page.locator(".ContentTitle__title--e3qXt").all()

for title in titles:
    print(title.text_content())
```

### 브라우저 종료

```python
browser.close()
pw.stop()
```

- 작업이 끝나면 반드시 브라우저를 닫고 Playwright를 종료해야 한다
- 종료하지 않으면 브라우저 프로세스가 백그라운드에 남아 메모리를 차지한다

### 전체 코드

```python
from playwright.sync_api import sync_playwright

# 1. Playwright 시작 및 브라우저 실행
pw = sync_playwright().start()
browser = pw.chromium.launch(headless=False)
page = browser.new_page()

# 2. 네이버 웹툰 페이지 이동
page.goto("https://comic.naver.com/webtoon?tab=mon")
page.wait_for_load_state("networkidle")

# 3. 웹툰 목록 추출 (선택자는 개발자 도구에서 확인)
items = page.locator(".ContentTitle__title--e3qXt").all()

print(f"웹툰 목록 ({len(items)}건)")
print("=" * 40)

for i, item in enumerate(items, 1):
    print(f"{i}. {item.text_content()}")

# 4. 브라우저 종료
browser.close()
pw.stop()
```

---

## 브라우저 조작하기

Playwright는 단순 데이터 추출 외에도 클릭, 입력, 스크롤 등 브라우저 조작이 가능하다.

### 클릭

```python
page.locator("a.link").click()
```

### 텍스트 입력

```python
page.locator("input.search").fill("검색어")
```

### 키보드 입력

```python
page.keyboard.press("Enter")
```

### 스크롤

```python
page.mouse.wheel(0, 500)   # 아래로 500px 스크롤
```

### 스크린샷

```python
page.screenshot(path="screenshot.png")
```

---

## requests + bs4 vs Playwright 비교

- `requests` + `BeautifulSoup`
    - 정적 HTML만 가져올 수 있다
    - 속도가 빠르다
    - 브라우저를 실행하지 않아 가볍다
    - 동적 페이지에서는 사용할 수 없다
- `Playwright`
    - 실제 브라우저를 실행하여 JavaScript 렌더링 후의 페이지를 가져온다
    - 클릭, 스크롤 등 브라우저 조작이 가능하다
    - 속도가 상대적으로 느리다
    - 동적 페이지에서 사용할 수 있다

---

### User-Agent

- HTTP 요청을 보낼 때 "나는 어떤 브라우저/프로그램이다" 라고 알려주는 정보이다
- `requests`의 기본 User-Agent는 `"python-requests/2.x.x"`이다
- 일부 사이트는 이를 감지하여 봇 요청을 차단한다
- 실제 브라우저처럼 보이도록 User-Agent를 설정하면 차단을 피할 수 있다

```python
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) ..."
}
response = requests.get(url, headers=headers)
```

### robots.txt

- 웹사이트마다 크롤링 허용/차단 범위를 명시한 파일이다
- `https://사이트주소/robots.txt`로 확인 가능하다
- 예: `https://www.naver.com/robots.txt`
- `Disallow`로 표시된 경로는 크롤링이 금지된 영역이다

### 법적/윤리적 고려사항

- 수집한 데이터를 상업적으로 사용하면 법적 문제가 될 수 있다
- 짧은 시간에 대량 요청을 보내면 서버에 부담을 주어 차단될 수 있다
- 개인정보가 포함된 데이터 수집은 개인정보보호법 위반 가능성이 있다
- 학습/연구 목적의 소규모 수집은 일반적으로 허용되는 범위이다

---

## 실습

1. 다음 사이트에서 책의 제목을 스크래핑해 출력해보자
    
    https://www.yes24.com/product/category/daybestseller?categoryNumber=001&pageNumber=1&pageSize=24&type=day